train_b_2nd$XGB_SOFTMAX <- pred_b_softmax
META_XGB_MULSOFT <- as.data.frame(t(matrix(pred_b_softprob, 8, nrow(train_b))))
names(META_XGB_MULSOFT) <- paste('META_XGB_MUL_', 1:8, sep = '')
META_XGB_MULSOFT_MLOG <- as.data.frame(t(matrix(pred_b_softprob_mlog, 8, nrow(train_b))))
names(META_XGB_MULSOFT_MLOG) <- paste('META_XGB_MUL_MLOG_', 1:8, sep = '')
train_b_2nd <- cbind(train_b_2nd, META_XGB_MULSOFT, META_XGB_MULSOFT_MLOG)
save(train_2nd, train_a_2nd, train_b_2nd, test, file= 'data/fin_train_test_stack_feat_2.RData')
ScoreQuadraticWeightedKappa(round(pred_b_rmse),train_b$Response)
ScoreQuadraticWeightedKappa(round(pred_a_rmse),train_a$Response)
ScoreQuadraticWeightedKappa(round(pred_b_kappa),train_b$Response)
ScoreQuadraticWeightedKappa(round(pred_a_kappa),train_a$Response)
ScoreQuadraticWeightedKappa(round(pred_b_softmax),train_b$Response)
ScoreQuadraticWeightedKappa(round(pred_a_softmax),train_a$Response)
ScoreQuadraticWeightedKappa(round(pred_b_softmax_mlog),train_b$Response)
ScoreQuadraticWeightedKappa(round(pred_a_softmax_mlog),train_a$Response)
setwd('/Users/ivanliu/Downloads/Prudential-Life-Insurance-Assessment')
library(readr)
library(xgboost)
library(Metrics)
library(Hmisc)
rm(list=ls());gc()
# load('data/fin_train_test_stack_feat.RData')
load('data/fin_train_test_stack_feat_2.RData')
evalerror = function(preds, dtrain) {
labels <- getinfo(dtrain, "label")
err <- ScoreQuadraticWeightedKappa(as.numeric(labels),as.numeric(round(preds)))
return(list(metric = "kappa", value = err))
}
evalerror_2 = function(x = seq(1.5, 7.5, by = 1), preds, labels) {
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(as.numeric(labels), preds, 1, 8)
return(-err)
}
setwd('/Users/ivanliu/Downloads/Prudential-Life-Insurance-Assessment')
library(readr)
library(xgboost)
library(Metrics)
library(Hmisc)
rm(list=ls());gc()
# load('data/fin_train_test_stack_feat.RData')
load('data/fin_train_test_stack_feat_2.RData')
evalerror = function(preds, dtrain) {
labels <- getinfo(dtrain, "label")
err <- ScoreQuadraticWeightedKappa(as.numeric(labels),as.numeric(round(preds)))
return(list(metric = "kappa", value = err))
}
evalerror_2 = function(x = seq(1.5, 7.5, by = 1), preds, labels) {
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(as.numeric(labels), preds, 1, 8)
return(-err)
}
set.seed(23)
dropitems <- c('Id','Response')
feature.names <- names(train)[!names(train) %in% dropitems]
### Num stack features
dtrain        <- xgb.DMatrix(data=data.matrix(train[,feature.names]),label=train$Response)
dtrain_a      <- xgb.DMatrix(data=data.matrix(train_a[,feature.names]),label=train_a$Response)
dtrain_b      <- xgb.DMatrix(data=data.matrix(train_b[,feature.names]),label=train_b$Response)
dtest         <- xgb.DMatrix(data=data.matrix(test[,feature.names]),label=test$Response)
watchlist     <- list(val=dtrain,train=dtrain)
watchlist_ab  <- list(val=dtrain_b,train=dtrain_a)
watchlist_ba  <- list(val=dtrain_a,train=dtrain_b)
clf <- xgb.train(data                = dtrain_a, # dtrain_a, dtrain_b, dtrain
nrounds             = 800,
early.stop.round    = 200,
watchlist           = watchlist_ab, # watchlist_ab, watchlist_ba, watchlist
# feval               = evalerror,
eval_metric         = 'rmse',
maximize            = F,
verbose             = 1,
objective           = "reg:linear",
booster             = "gbtree",
eta                 = 0.035,
max_depth           = 6,
min_child_weight    = 3,
subsample           = 0.9,
colsample           = 0.67,
print.every.n       = 10
)
### Make predictions
validPreds <- predict(clf, data.matrix(validation[,feature.names]))  # feature.names.stack
validScore <- ScoreQuadraticWeightedKappa(round(validPreds),validation$Response)
evalerror_2(preds = validPreds, labels = validation$Response)
setwd('/Users/ivanliu/Downloads/Prudential-Life-Insurance-Assessment')
library(readr)
library(xgboost)
library(Metrics)
library(Hmisc)
rm(list=ls());gc()
# load('data/fin_train_test_stack_feat.RData')
load('data/fin_train_test_stack_feat_2.RData')
evalerror = function(preds, dtrain) {
labels <- getinfo(dtrain, "label")
err <- ScoreQuadraticWeightedKappa(as.numeric(labels),as.numeric(round(preds)))
return(list(metric = "kappa", value = err))
}
evalerror_2 = function(x = seq(1.5, 7.5, by = 1), preds, labels) {
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(as.numeric(labels), preds, 1, 8)
return(-err)
}
ls()
setwd('/Users/ivanliu/Downloads/Prudential-Life-Insurance-Assessment')
library(readr)
library(xgboost)
library(Metrics)
library(Hmisc)
rm(list=ls());gc()
# load('data/fin_train_test_stack_feat.RData')
load('data/fin_train_test_stack_feat_2.RData')
evalerror = function(preds, dtrain) {
labels <- getinfo(dtrain, "label")
err <- ScoreQuadraticWeightedKappa(as.numeric(labels),as.numeric(round(preds)))
return(list(metric = "kappa", value = err))
}
evalerror_2 = function(x = seq(1.5, 7.5, by = 1), preds, labels) {
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(as.numeric(labels), preds, 1, 8)
return(-err)
}
set.seed(23)
dropitems <- c('Id','Response')
feature.names <- names(train_2nd)[!names(train_2nd) %in% dropitems]
### Num stack features
dtrain        <- xgb.DMatrix(data=data.matrix(train_2nd[,feature.names]),label=train_2nd$Response)
dtrain_a      <- xgb.DMatrix(data=data.matrix(train_a_2nd[,feature.names]),label=train_a_2nd$Response)
dtrain_b      <- xgb.DMatrix(data=data.matrix(train_b_2nd[,feature.names]),label=train_b_2nd$Response)
dtest         <- xgb.DMatrix(data=data.matrix(test[,feature.names]),label=test$Response)
watchlist     <- list(val=dtrain,train=dtrain)
watchlist_ab  <- list(val=dtrain_b,train=dtrain_a)
watchlist_ba  <- list(val=dtrain_a,train=dtrain_b)
feature.names <- names(train_2nd)[!names(train_2nd) %in% dropitems]
feature.names
dtrain        <- xgb.DMatrix(data=data.matrix(train_2nd[,feature.names]),label=train_2nd$Response)
dtrain_a      <- xgb.DMatrix(data=data.matrix(train_a_2nd[,feature.names]),label=train_a_2nd$Response)
head(train_a_2nd)
train_a_2nd$Response
head(train_a_2nd[,feature.names])
names(train_a_2nd)
names(train_2nd)
setwd('/Users/ivanliu/Downloads/Prudential-Life-Insurance-Assessment')
library(readr)
library(xgboost)
library(Metrics)
library(Hmisc)
rm(list=ls());gc()
# load('data/fin_train_test_stack_feat.RData')
load('data/fin_train_test_stack_feat_2.RData')
evalerror = function(preds, dtrain) {
labels <- getinfo(dtrain, "label")
err <- ScoreQuadraticWeightedKappa(as.numeric(labels),as.numeric(round(preds)))
return(list(metric = "kappa", value = err))
}
evalerror_2 = function(x = seq(1.5, 7.5, by = 1), preds, labels) {
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(as.numeric(labels), preds, 1, 8)
return(-err)
}
set.seed(23)
dropitems <- c('Id','Response', 'XGB_SOFTMAX_MLOG')
feature.names <- names(train_2nd)[!names(train_2nd) %in% dropitems]
### Num stack features
dtrain        <- xgb.DMatrix(data=data.matrix(train_2nd[,feature.names]),label=train_2nd$Response)
dtrain_a      <- xgb.DMatrix(data=data.matrix(train_a_2nd[,feature.names]),label=train_a_2nd$Response)
dtrain_b      <- xgb.DMatrix(data=data.matrix(train_b_2nd[,feature.names]),label=train_b_2nd$Response)
dtest         <- xgb.DMatrix(data=data.matrix(test[,feature.names]),label=test$Response)
watchlist     <- list(val=dtrain,train=dtrain)
watchlist_ab  <- list(val=dtrain_b,train=dtrain_a)
watchlist_ba  <- list(val=dtrain_a,train=dtrain_b)
clf <- xgb.train(data                = dtrain_a, # dtrain_a, dtrain_b, dtrain
nrounds             = 800,
early.stop.round    = 200,
watchlist           = watchlist_ab, # watchlist_ab, watchlist_ba, watchlist
# feval               = evalerror,
eval_metric         = 'rmse',
maximize            = F,
verbose             = 1,
objective           = "reg:linear",
booster             = "gbtree",
eta                 = 0.035,
max_depth           = 6,
min_child_weight    = 3,
subsample           = 0.9,
colsample           = 0.67,
print.every.n       = 10
)
validPreds <- predict(clf, data.matrix(validation[,feature.names]))  # feature.names.stack
pred_b_rmse <- predict(clf, data.matrix(train_b_2nd[,feature.names]))
ScoreQuadraticWeightedKappa(round(pred_b_rmse),train_b_2nd$Response)
validScore <- ScoreQuadraticWeightedKappa(round(pred_b_rmse),train_b_2nd$Response)
optCuts = optim(seq(1.5, 7.5, by = 1), evalerror_2, preds = pred_b_rmse, labels = train_b_2nd$Response,
method = 'Nelder-Mead', control = list(maxit = 30000, trace = TRUE, REPORT = 500))
pred_b_rmse <- predict(clf, data.matrix(train_b_2nd[,feature.names]))
clf <- xgb.train(data                = dtrain_b, # dtrain_a, dtrain_b, dtrain
nrounds             = 800,
early.stop.round    = 200,
watchlist           = watchlist_ba, # watchlist_ab, watchlist_ba, watchlist
# feval               = evalerror,
eval_metric         = 'rmse',
maximize            = F,
verbose             = 1,
objective           = "reg:linear",
booster             = "gbtree",
eta                 = 0.035,
max_depth           = 6,
min_child_weight    = 3,
subsample           = 0.9,
colsample           = 0.67,
print.every.n       = 10
)
pred_a_rmse <- predict(clf, data.matrix(train_a_2nd[,feature.names]))
clf <- xgb.train(data                = dtrain_a, # dtrain_a, dtrain_b, dtrain
nrounds             = 800,
early.stop.round    = 200,
watchlist           = watchlist_ab, # watchlist_ab, watchlist_ba, watchlist
feval               = evalerror,
# eval_metric         = 'rmse',
maximize            = T,
verbose             = 1,
objective           = "reg:linear",
booster             = "gbtree",
eta                 = 0.035,
max_depth           = 6,
min_child_weight    = 3,
subsample           = 0.9,
colsample           = 0.67,
print.every.n       = 10
)
pred_b_kap <- predict(clf, data.matrix(train_b_2nd[,feature.names]))
clf <- xgb.train(data                = dtrain_b, # dtrain_a, dtrain_b, dtrain
nrounds             = 800,
early.stop.round    = 200,
watchlist           = watchlist_ba, # watchlist_ab, watchlist_ba, watchlist
feval               = evalerror,
# eval_metric         = 'rmse',
maximize            = T,
verbose             = 1,
objective           = "reg:linear",
booster             = "gbtree",
eta                 = 0.035,
max_depth           = 6,
min_child_weight    = 3,
subsample           = 0.9,
colsample           = 0.67,
print.every.n       = 10
)
pred_a_kap <- predict(clf, data.matrix(train_a_2nd[,feature.names]))
validPreds <- (c(pred_a_rmse,pred_b_rmse) + c(pred_a_kap,pred_b_kap))/2
library(mlr)
optCuts = optim(seq(1.5, 7.5, by = 1), evalerror_2, preds = validPreds, labels = train_2nd$Response,
method = 'Nelder-Mead', control = list(maxit = 30000, trace = TRUE, REPORT = 500))
validPredsOptim = as.numeric(Hmisc::cut2(validPreds, c(-Inf, optCuts$par, Inf))); table(validPredsOptim)
evalerror_2(preds = validPredsOptim, labels = train_2nd$Response)
optCuts
clf <- xgb.train(data                = dtrain, # dtrain_a, dtrain_b, dtrain
nrounds             = 200,
early.stop.round    = 200,
watchlist           = watchlist, # watchlist_ab, watchlist_ba, watchlist
feval               = evalerror,
# eval_metric         = 'rmse',
maximize            = T,
verbose             = 1,
objective           = "reg:linear",
booster             = "gbtree",
eta                 = 0.035,
max_depth           = 6,
min_child_weight    = 3,
subsample           = 0.9,
colsample           = 0.67,
print.every.n       = 10
)
test_kap <- predict(clf, data.matrix(train_2nd[,feature.names]))
clf <- xgb.train(data                = dtrain, # dtrain_a, dtrain_b, dtrain
nrounds             = 200,
early.stop.round    = 200,
watchlist           = watchlist, # watchlist_ab, watchlist_ba, watchlist
# feval               = evalerror,
eval_metric         = 'rmse',
maximize            = F,
verbose             = 1,
objective           = "reg:linear",
booster             = "gbtree",
eta                 = 0.035,
max_depth           = 6,
min_child_weight    = 3,
subsample           = 0.9,
colsample           = 0.67,
print.every.n       = 10
)
clf <- xgb.train(data                = dtrain, # dtrain_a, dtrain_b, dtrain
nrounds             = 120,
early.stop.round    = 200,
watchlist           = watchlist, # watchlist_ab, watchlist_ba, watchlist
# feval               = evalerror,
eval_metric         = 'rmse',
maximize            = F,
verbose             = 1,
objective           = "reg:linear",
booster             = "gbtree",
eta                 = 0.035,
max_depth           = 6,
min_child_weight    = 3,
subsample           = 0.9,
colsample           = 0.67,
print.every.n       = 10
)
test_rmse <- predict(clf, data.matrix(train_2nd[,feature.names]))
testPreds <- (test_kap + test_rmse)/2
PredsOptim = as.numeric(Hmisc::cut2(testPreds, c(-Inf, optCuts$par, Inf))); table(PredsOptim)
submission <- data.frame(Id=test$Id, Response = PredsOptim)
length(testPreds)
length(test$Id)
test_rmse <- predict(clf, data.matrix(test[,feature.names]))
test_rmse
clf <- xgb.train(data                = dtrain, # dtrain_a, dtrain_b, dtrain
nrounds             = 200,
early.stop.round    = 200,
watchlist           = watchlist, # watchlist_ab, watchlist_ba, watchlist
feval               = evalerror,
# eval_metric         = 'rmse',
maximize            = T,
verbose             = 1,
objective           = "reg:linear",
booster             = "gbtree",
eta                 = 0.035,
max_depth           = 6,
min_child_weight    = 3,
subsample           = 0.9,
colsample           = 0.67,
print.every.n       = 10
)
test_kap <- predict(clf, data.matrix(test[,feature.names]))
testPreds <- (test_kap + test_rmse)/2
PredsOptim = as.numeric(Hmisc::cut2(testPreds, c(-Inf, optCuts$par, Inf))); table(PredsOptim)
submission <- data.frame(Id=test$Id, Response = PredsOptim)
head(submission)
write_csv(submission, "submission_xgb_stack_20160201_1.csv")
head(submission, 30)
setwd('/Users/ivanliu/Downloads/Prudential-Life-Insurance-Assessment')
library(readr)
library(xgboost)
library(Metrics)
library(Hmisc)
rm(list=ls());gc()
# load('data/fin_train_test_validation.RData')
load('data/fin_train_test.RData')
evalerror = function(preds, dtrain) {
labels <- getinfo(dtrain, "label")
err <- ScoreQuadraticWeightedKappa(as.numeric(labels),as.numeric(round(preds)))
return(list(metric = "kappa", value = err))
}
evalerror_2 = function(x = seq(1.5, 7.5, by = 1), preds, labels) {
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(as.numeric(labels), preds, 1, 8)
return(-err)
}
set.seed(23)
dropitems <- c('Id','Response')
feature.names <- names(train)[!names(train) %in% dropitems]
### Num stack features
dtrain        <- xgb.DMatrix(data=data.matrix(train[,feature.names]),label=train$Response)
dtrain_a      <- xgb.DMatrix(data=data.matrix(train_a[,feature.names]),label=train_a$Response)
dtrain_b      <- xgb.DMatrix(data=data.matrix(train_b[,feature.names]),label=train_b$Response)
dtest         <- xgb.DMatrix(data=data.matrix(test[,feature.names]),label=test$Response)
watchlist     <- list(val=dtrain,train=dtrain)
watchlist_ab  <- list(val=dtrain_b,train=dtrain_a)
watchlist_ba  <- list(val=dtrain_a,train=dtrain_b)
# Feat1
clf <- xgb.train(data = dtrain_a, eval_metric = 'rmse',
early.stop.round = 200, watchlist = watchlist_ab, maximize = F,
verbose = 1, objective = "reg:linear",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 700, colsample = 0.67, print.every.n = 10
)
pred_b_rmse <- predict(clf, data.matrix(train_b[,feature.names]))
clf <- xgb.train(data = dtrain_b, eval_metric = 'rmse',
early.stop.round = 200, watchlist = watchlist_ba, maximize = F,
verbose = 1, objective = "reg:linear",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 700, colsample = 0.67, print.every.n = 10
)
pred_a_rmse <- predict(clf, data.matrix(train_a[,feature.names]))
clf <- xgb.train(data = dtrain, eval_metric = 'rmse',
early.stop.round = 200, watchlist = watchlist, maximize = F,
verbose = 1, objective = "reg:linear",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 700, colsample = 0.67, print.every.n = 10
)
pred_test_rmse <- predict(clf, data.matrix(test[,feature.names]))
# Feat2
clf <- xgb.train(data = dtrain_a, feval = evalerror,
early.stop.round = 200, watchlist = watchlist_ab, maximize = T,
verbose = 1, objective = "reg:linear",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 700, colsample = 0.67, print.every.n = 10
)
pred_b_kappa <- predict(clf, data.matrix(train_b[,feature.names]))
clf <- xgb.train(data = dtrain_b, feval = evalerror,
early.stop.round = 200, watchlist = watchlist_ba, maximize = T,
verbose = 1, objective = "reg:linear",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 700, colsample = 0.67, print.every.n = 10
)
pred_a_kappa <- predict(clf, data.matrix(train_a[,feature.names]))
clf <- xgb.train(data = dtrain, feval = evalerror,
early.stop.round = 200, watchlist = watchlist, maximize = T,
verbose = 1, objective = "reg:linear",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 700, colsample = 0.67, print.every.n = 10
)
pred_test_kappa <- predict(clf, data.matrix(test[,feature.names]))
### Cate stack features
dtrain        <- xgb.DMatrix(data=data.matrix(train[,feature.names]),label=train$Response-1)
dtrain_a      <- xgb.DMatrix(data=data.matrix(train_a[,feature.names]),label=train_a$Response-1)
dtrain_b      <- xgb.DMatrix(data=data.matrix(train_b[,feature.names]),label=train_b$Response-1)
dtest         <- xgb.DMatrix(data=data.matrix(test[,feature.names]),label=test$Response-1)
watchlist     <- list(val=dtrain,train=dtrain)
watchlist_ab  <- list(val=dtrain_b,train=dtrain_a)
watchlist_ba  <- list(val=dtrain_a,train=dtrain_b)
# Feat3
clf <- xgb.train(data = dtrain_a, eval_metric = 'merror',
early.stop.round = 200, watchlist = watchlist_ab, maximize = F,
verbose = 1, objective = "multi:softmax",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 500, colsample = 0.7, print.every.n = 10 ,num_class = 8
)
pred_b_softmax <- predict(clf, data.matrix(train_b[,feature.names]))
clf <- xgb.train(data = dtrain_b, eval_metric = 'merror',
early.stop.round = 200, watchlist = watchlist_ba, maximize = F,
verbose = 1, objective = "multi:softmax",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 500, colsample = 0.7, print.every.n = 10 ,num_class = 8
)
pred_a_softmax <- predict(clf, data.matrix(train_a[,feature.names]))
clf <- xgb.train(data = dtrain, eval_metric = 'merror',
early.stop.round = 200, watchlist = watchlist, maximize = F,
verbose = 1, objective = "multi:softmax",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 500, colsample = 0.7, print.every.n = 10 ,num_class = 8
)
pred_test_softmax <- predict(clf, data.matrix(test[,feature.names]))
# Feat4
clf <- xgb.train(data = dtrain_a, eval_metric = 'merror',
early.stop.round = 200, watchlist = watchlist_ab, maximize = F,
verbose = 1, objective = "multi:softprob",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 500, colsample = 0.7, print.every.n = 10 ,num_class = 8
)
pred_b_softprob <- predict(clf, data.matrix(train_b[,feature.names]))
clf <- xgb.train(data = dtrain_b, eval_metric = 'merror',
early.stop.round = 200, watchlist = watchlist_ba, maximize = F,
verbose = 1, objective = "multi:softprob",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 800, colsample = 0.7, print.every.n = 10 ,num_class = 8
)
pred_a_softprob <- predict(clf, data.matrix(train_a[,feature.names]))
clf <- xgb.train(data = dtrain, eval_metric = 'merror',
early.stop.round = 200, watchlist = watchlist, maximize = F,
verbose = 1, objective = "multi:softprob",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 800, colsample = 0.7, print.every.n = 10 ,num_class = 8
)
pred_test_softprob <- predict(clf, data.matrix(test[,feature.names]))
# Feat5
clf <- xgb.train(data = dtrain_a, eval_metric = 'mlogloss',
early.stop.round = 200, watchlist = watchlist_ab, maximize = F,
verbose = 1, objective = "multi:softmax",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 800, colsample = 0.7, print.every.n = 10 ,num_class = 8
)
pred_b_softmax_mlog <- predict(clf, data.matrix(train_b[,feature.names]))
clf <- xgb.train(data = dtrain_b, eval_metric = 'mlogloss',
early.stop.round = 200, watchlist = watchlist_ba, maximize = F,
verbose = 1, objective = "multi:softmax",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 500, colsample = 0.7, print.every.n = 10 ,num_class = 8
)
pred_a_softmax_mlog <- predict(clf, data.matrix(train_a[,feature.names]))
clf <- xgb.train(data = dtrain, eval_metric = 'mlogloss',
early.stop.round = 200, watchlist = watchlist, maximize = F,
verbose = 1, objective = "multi:softmax",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 500, colsample = 0.7, print.every.n = 10 ,num_class = 8
)
pred_test_softmax_mlog <- predict(clf, data.matrix(test[,feature.names]))
# Feat6
clf <- xgb.train(data = dtrain_a, eval_metric = 'mlogloss',
early.stop.round = 200, watchlist = watchlist_ab, maximize = F,
verbose = 1, objective = "multi:softprob",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 500, colsample = 0.7, print.every.n = 10 ,num_class = 8
)
pred_b_softprob_mlog <- predict(clf, data.matrix(train_b[,feature.names]))
clf <- xgb.train(data = dtrain_b, eval_metric = 'mlogloss',
early.stop.round = 200, watchlist = watchlist_ba, maximize = F,
verbose = 1, objective = "multi:softprob",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 500, colsample = 0.7, print.every.n = 10 ,num_class = 8
)
pred_a_softprob_mlog <- predict(clf, data.matrix(train_a[,feature.names]))
clf <- xgb.train(data = dtrain, eval_metric = 'mlogloss',
early.stop.round = 200, watchlist = watchlist, maximize = F,
verbose = 1, objective = "multi:softprob",
booste = "gbtree", eta = 0.035, max_depth = 6, min_child_weight = 3, subsample = 0.8,
nrounds = 500, colsample = 0.7, print.every.n = 10 ,num_class = 8
)
