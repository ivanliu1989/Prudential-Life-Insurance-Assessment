label       = train_20$Response,
nrounds     = 100,
objective   = "reg:linear",
eval_metric = "rmse")
# just for keeping track of how things went...
# run prediction on training set so we can add the value to our output filename
validPreds <- predict(clf, data.matrix(validation_20[,feature.names])) # validation_20, validation_10
validScore <- ScoreQuadraticWeightedKappa(round(validPreds),validation_20$Response) # validation_20, validation_10
evalerror2(validPreds, validation_20$Response)
clf <- xgboost(data                = data.matrix(train_20[,feature.names]),
label               = train_20$Response,
nrounds             = 100,
objective           = "reg:linear",
eval_metric         = "rmse",
lambda              = 0.001,
alpha               = 0.001,
early.stop.round    = myEarlyStopRound,
watchlist           = watchlist,
maximize            = F,
verbose             = 1)
validPreds <- predict(clf, data.matrix(validation_20[,feature.names])) # validation_20, validation_10
validScore <- ScoreQuadraticWeightedKappa(round(validPreds),validation_20$Response) # validation_20, validation_10
evalerror2(validPreds, validation_20$Response)
?xgboost
clf <- xgb.train(data                = dtrain,
nrounds             = 100,
early.stop.round    = 50,
watchlist           = watchlist,
feval               = evalerror,
maximize            = TRUE,
verbose             = 1,
objective           = "reg:linear",
maximize            = T
)
clf <- xgb.train(data                = dtrain,
nrounds             = 100,
early.stop.round    = 50,
watchlist           = watchlist,
feval               = evalerror,
maximize            = TRUE,
verbose             = 1,
objective           = "reg:linear"
)
?xgb.train
clf <- xgb.train(data                = dtrain,
nrounds             = 100,
early.stop.round    = 50,
watchlist           = watchlist,
feval               = evalerror,
maximize            = TRUE,
verbose             = 1,
objective           = "reg:linear",
booster             = "gbtree"
)
validPreds <- predict(clf, data.matrix(validation_20[,feature.names])) # validation_20, validation_10
validScore <- ScoreQuadraticWeightedKappa(round(validPreds),validation_20$Response) # validation_20, validation_10
evalerror2(validPreds, validation_20$Response)
clf <- xgb.train(data                = dtrain,
nrounds             = 1000,
early.stop.round    = 100,
watchlist           = watchlist,
feval               = evalerror,
maximize            = TRUE,
verbose             = 1,
objective           = "reg:linear",
booster             = "gbtree",
eta                 = 0.2,
gamma               = 0.01,
max_depth           = 6,
min_child_weight    = 50,
subsample           = 0.9,
colsample           = 0.7,
print.every.n       = 10
)
clf <- xgb.train(data                = dtrain,
nrounds             = 1000,
early.stop.round    = 100,
watchlist           = watchlist,
feval               = evalerror,
maximize            = TRUE,
verbose             = 1,
objective           = "reg:linear",
booster             = "gbtree",
eta                 = 0.25,
gamma               = 0.1,
max_depth           = 8,
min_child_weight    = 50,
subsample           = 0.9,
colsample           = 0.7,
print.every.n       = 10
)
library(mlr)
optCuts = optim(seq(1.5, 7.5, by = 1), evalerror_2, preds = validPreds, labels = validation_20$Response)
evalerror_2 = function(x = seq(1.5, 7.5, by = 1), preds, labels) {
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(as.numeric(labels), preds, 1, 8)
return(list(metric = "kappa", value = err))
}
optCuts = optim(seq(1.5, 7.5, by = 1), evalerror_2, preds = validPreds, labels = validation_20$Response)
evalerror_2 = function(x = seq(1.5, 7.5, by = 1), preds, labels) {
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(as.numeric(labels), preds, 1, 8)
return(err)
}
evalerror_2(validPreds, validation_20$Response)
evalerror_2(preds = validPreds, labels = validation_20$Response)
library(mlr)
optCuts = optim(seq(1.5, 7.5, by = 1), evalerror_2, preds = validPreds, labels = validation_20$Response)
optCuts
optCuts$par
validPredsOptim = as.numeric(Hmisc::cut2(validPreds, c(-Inf, optCuts$par, Inf)))
table(validPredsOptim)
evalerror_2(preds = validPredsOptim, labels = validation_20$Response)
?optim
evalerror_2 = function(x = seq(1.5, 7.5, by = 1), preds, labels) {
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(as.numeric(labels), preds, 1, 8)
return(-err)
}
optCuts = optim(seq(1.5, 7.5, by = 1), evalerror_2, preds = validPreds, labels = validation_20$Response)
optCuts
validPredsOptim = as.numeric(Hmisc::cut2(validPreds, c(-Inf, optCuts$par, Inf)))
table(validPredsOptim)
evalerror_2(preds = validPredsOptim, labels = validation_20$Response)
setwd('/Users/ivanliu/Downloads/Prudential-Life-Insurance-Assessment')
library(readr)
library(xgboost)
library(Metrics)
library(Hmisc)
rm(list=ls());gc()
# [Tune] Result: eta=0.277; colsample_bytree=0.954; subsample=0.813 : SQWK.test.mean=0.602
####################################################################################################
# FUNCTION / VARIABLE DECLARTIONS
####################################################################################################
# evaluation function that we'll use for "feval" in xgb.train...
# evalerror <- function(preds, dtrain) {
#     labels <- getinfo(dtrain, "label")
#     err <- ScoreQuadraticWeightedKappa(as.numeric(labels),as.numeric(round(preds)))
#     return(list(metric = "kappa", value = err))
# }
evalerror = function(preds, dtrain) {
x = seq(1.5, 7.5, by = 1)
labels <- getinfo(dtrain, "label")
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(as.numeric(labels), preds, 1, 8)
return(list(metric = "kappa", value = err))
}
evalerror_2 = function(x = seq(1.5, 7.5, by = 1), preds, labels) {
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(as.numeric(labels), preds, 1, 8)
return(-err)
}
####################################################################################################
# MAINLINE
####################################################################################################
set.seed(1989)
cat("read train and test data...\n")
load("data/cleaned_datasets.RData")
# load("data/cleaned_datasets_imputed.RData")
# load("data/cleaned_datasets_no_encoded.RData")
feature.names <- names(train)[2:909] #132 ncol(train)-1
# response values are in the range [1:8] ... make it [0:7] for xgb softmax....
# train_20$Response = train_20$Response - 1 # train_20, train_10
cat("create dval/dtrain/watchlist...\n")
dval       <- xgb.DMatrix(data=data.matrix(validation_20[,feature.names]),label=validation_20$Response) # validation_20, validation_10
dtrain     <- xgb.DMatrix(data=data.matrix(train_20[,feature.names]),label=train_20$Response) # train_20, train_10
watchlist  <- list(val=dval,train=dtrain)
cat("running xgboost...\n")
clf <- xgb.train(data                = dtrain,
nrounds             = 1000,
early.stop.round    = 100,
watchlist           = watchlist,
feval               = evalerror,
maximize            = TRUE,
verbose             = 1,
objective           = "reg:linear",
booster             = "gbtree",
eta                 = 0.2,
gamma               = 0.05,
max_depth           = 12,
min_child_weight    = 100,
subsample           = 0.9,
colsample           = 0.7,
print.every.n       = 10
)
# just for keeping track of how things went...
# run prediction on training set so we can add the value to our output filename
validPreds <- predict(clf, data.matrix(validation_20[,feature.names])) # validation_20, validation_10
validScore <- ScoreQuadraticWeightedKappa(round(validPreds),validation_20$Response) # validation_20, validation_10
evalerror_2(preds = validPreds, labels = validation_20$Response)
# Find optimal cutoff
library(mlr)
optCuts = optim(seq(1.5, 7.5, by = 1), evalerror_2, preds = validPreds, labels = validation_20$Response)
optCuts
validPredsOptim = as.numeric(Hmisc::cut2(validPreds, c(-Inf, optCuts$par, Inf)))
table(validPredsOptim)
evalerror_2(preds = validPredsOptim, labels = validation_20$Response)
# cleaned_datasets_imputed.RData - 0.5959
# cleaned_datasets_no_encoded.RData - 0.5969
# cleaned_datasets.RData - 0.5967 | 0.6047 (no tsne) | 0.61219
# optimal cutoff - 0.6442975
setwd('/Users/ivanliu/Downloads/Prudential-Life-Insurance-Assessment')
library(readr)
library(xgboost)
library(Metrics)
library(Hmisc)
rm(list=ls());gc()
# [Tune] Result: eta=0.277; colsample_bytree=0.954; subsample=0.813 : SQWK.test.mean=0.602
####################################################################################################
# FUNCTION / VARIABLE DECLARTIONS
####################################################################################################
# evaluation function that we'll use for "feval" in xgb.train...
# evalerror <- function(preds, dtrain) {
#     labels <- getinfo(dtrain, "label")
#     err <- ScoreQuadraticWeightedKappa(as.numeric(labels),as.numeric(round(preds)))
#     return(list(metric = "kappa", value = err))
# }
evalerror = function(preds, dtrain) {
x = seq(1.5, 7.5, by = 1)
labels <- getinfo(dtrain, "label")
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(as.numeric(labels), preds, 1, 8)
return(list(metric = "kappa", value = err))
}
evalerror_2 = function(x = seq(1.5, 7.5, by = 1), preds, labels) {
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(as.numeric(labels), preds, 1, 8)
return(-err)
}
####################################################################################################
# MAINLINE
####################################################################################################
set.seed(1989)
cat("read train and test data...\n")
load("data/cleaned_datasets.RData")
# load("data/cleaned_datasets_imputed.RData")
# load("data/cleaned_datasets_no_encoded.RData")
feature.names <- names(train)[2:909] #132 ncol(train)-1
# response values are in the range [1:8] ... make it [0:7] for xgb softmax....
# train_20$Response = train_20$Response - 1 # train_20, train_10
cat("create dval/dtrain/watchlist...\n")
dval       <- xgb.DMatrix(data=data.matrix(validation_20[,feature.names]),label=validation_20$Response) # validation_20, validation_10
dtrain     <- xgb.DMatrix(data=data.matrix(train_20[,feature.names]),label=train_20$Response) # train_20, train_10
watchlist  <- list(val=dval,train=dtrain)
cat("running xgboost...\n")
clf <- xgb.train(data                = dtrain,
nrounds             = 1000,
early.stop.round    = 200,
watchlist           = watchlist,
feval               = evalerror,
maximize            = TRUE,
verbose             = 1,
objective           = "reg:linear",
booster             = "gbtree",
eta                 = 0.15,
gamma               = 0.05,
max_depth           = 8,
min_child_weight    = 3,
subsample           = 0.8,
colsample           = 0.7,
print.every.n       = 10
)
# just for keeping track of how things went...
# run prediction on training set so we can add the value to our output filename
validPreds <- predict(clf, data.matrix(validation_20[,feature.names])) # validation_20, validation_10
validScore <- ScoreQuadraticWeightedKappa(round(validPreds),validation_20$Response) # validation_20, validation_10
evalerror_2(preds = validPreds, labels = validation_20$Response)
# Find optimal cutoff
library(mlr)
optCuts = optim(seq(1.5, 7.5, by = 1), evalerror_2, preds = validPreds, labels = validation_20$Response)
optCuts
validPredsOptim = as.numeric(Hmisc::cut2(validPreds, c(-Inf, optCuts$par, Inf)))
table(validPredsOptim)
evalerror_2(preds = validPredsOptim, labels = validation_20$Response)
optCuts
names(train)
setwd('/Users/ivanliu/Downloads/Prudential-Life-Insurance-Assessment')
library(readr)
library(xgboost)
library(Metrics)
library(Hmisc)
rm(list=ls());gc()
# [Tune] Result: eta=0.277; colsample_bytree=0.954; subsample=0.813 : SQWK.test.mean=0.602
####################################################################################################
# FUNCTION / VARIABLE DECLARTIONS
####################################################################################################
# evaluation function that we'll use for "feval" in xgb.train...
# evalerror <- function(preds, dtrain) {
#     labels <- getinfo(dtrain, "label")
#     err <- ScoreQuadraticWeightedKappa(as.numeric(labels),as.numeric(round(preds)))
#     return(list(metric = "kappa", value = err))
# }
evalerror = function(preds, dtrain) {
x = seq(1.5, 7.5, by = 1)
labels <- getinfo(dtrain, "label")
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(as.numeric(labels), preds, 1, 8)
return(list(metric = "kappa", value = err))
}
evalerror_2 = function(x = seq(1.5, 7.5, by = 1), preds, labels) {
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(as.numeric(labels), preds, 1, 8)
return(-err)
}
####################################################################################################
# MAINLINE
####################################################################################################
set.seed(1989)
cat("read train and test data...\n")
load("data/cleaned_datasets.RData")
# load("data/cleaned_datasets_imputed.RData")
# load("data/cleaned_datasets_no_encoded.RData")
feature.names <- names(train)[2:894] #132 ncol(train)-1
# response values are in the range [1:8] ... make it [0:7] for xgb softmax....
# train_20$Response = train_20$Response - 1 # train_20, train_10
cat("create dval/dtrain/watchlist...\n")
dval       <- xgb.DMatrix(data=data.matrix(validation_20[,feature.names]),label=validation_20$Response) # validation_20, validation_10
dtrain     <- xgb.DMatrix(data=data.matrix(train_20[,feature.names]),label=train_20$Response) # train_20, train_10
watchlist  <- list(val=dval,train=dtrain)
cat("running xgboost...\n")
clf <- xgb.train(data                = dtrain,
nrounds             = 1000,
early.stop.round    = 200,
watchlist           = watchlist,
feval               = evalerror,
maximize            = TRUE,
verbose             = 1,
objective           = "reg:linear",
booster             = "gbtree",
eta                 = 0.15,
gamma               = 0.05,
max_depth           = 8,
min_child_weight    = 3,
subsample           = 0.8,
colsample           = 0.7,
print.every.n       = 10
)
# just for keeping track of how things went...
# run prediction on training set so we can add the value to our output filename
validPreds <- predict(clf, data.matrix(validation_20[,feature.names])) # validation_20, validation_10
validScore <- ScoreQuadraticWeightedKappa(round(validPreds),validation_20$Response) # validation_20, validation_10
evalerror_2(preds = validPreds, labels = validation_20$Response)
# Find optimal cutoff
library(mlr)
optCuts = optim(seq(1.5, 7.5, by = 1), evalerror_2, preds = validPreds, labels = validation_20$Response)
optCuts
validPredsOptim = as.numeric(Hmisc::cut2(validPreds, c(-Inf, optCuts$par, Inf)))
table(validPredsOptim)
evalerror_2(preds = validPredsOptim, labels = validation_20$Response)
# cleaned_datasets_imputed.RData - 0.5959
# cleaned_datasets_no_encoded.RData - 0.5969
# cleaned_datasets.RData - 0.5967 | 0.6047 (no tsne) | 0.61219
# optimal cutoff - 0.6442975
setwd('/Users/ivanliu/Downloads/Prudential-Life-Insurance-Assessment')
library(Metrics)
library(Hmisc)
library(xgboost)
library(checkmate)
library(mlr)
library(data.table)
load("data/cleaned_datasets.RData")
# load("data/cleaned_datasets_imputed.RData")
# load("data/cleaned_datasets_no_encoded.RData")
## create mlr task and convert factors to dummy features
trainTask = makeRegrTask(data = train_20[,-1], target = "Response")
validTask = makeRegrTask(data = validation_20[,-1], target = "Response")
testTask = makeRegrTask(data = test[,-1], target = "Response")
trainTask
validTask
testTask
makeNumericParam("eta", lower = 0.1, upper = 0.3)
setwd('/Users/ivanliu/Downloads/Prudential-Life-Insurance-Assessment')
library(Metrics)
library(Hmisc)
library(xgboost)
library(checkmate)
library(mlr)
library(data.table)
load("data/cleaned_datasets.RData")
# load("data/cleaned_datasets_imputed.RData")
# load("data/cleaned_datasets_no_encoded.RData")
##################################################################
# 1.create mlr task and convert factors to dummy features ########
##################################################################
allTask = makeRegrTask(data = train[,-1], target = "Response")
trainTask = makeRegrTask(data = train_20[,-1], target = "Response")
validTask = makeRegrTask(data = validation_20[,-1], target = "Response")
testTask = makeRegrTask(data = test[,-1], target = "Response")
##############################
# 2.create mlr learner #######
##############################
set.seed(1989)
lrn = makeLearner("regr.xgboost")
lrn$par.vals = list(
nthread             = 5,
nrounds             = 800,
print.every.n       = 10,
objective           = "reg:linear"
)
lrn = makeLearner("regr.xgboost")
setwd('/Users/ivanliu/Downloads/Prudential-Life-Insurance-Assessment')
library(Metrics)
library(Hmisc)
library(xgboost)
library(checkmate)
library(mlr)
library(data.table)
load("data/cleaned_datasets.RData")
# load("data/cleaned_datasets_imputed.RData")
# load("data/cleaned_datasets_no_encoded.RData")
##################################################################
# 1.create mlr task and convert factors to dummy features ########
##################################################################
allTask = makeRegrTask(data = train[,-1], target = "Response")
trainTask = makeRegrTask(data = train_20[,-1], target = "Response")
validTask = makeRegrTask(data = validation_20[,-1], target = "Response")
testTask = makeRegrTask(data = test[,-1], target = "Response")
?makeLearner
lrn = makeLearner("regr.xgboost")
install.packages(c("gstat", "rgl"))
# Created by Giuseppe Casalicchio
library(Metrics)
library(Hmisc)
library(xgboost)
library(checkmate)
library(mlr)
library(data.table)
packageVersion("mlr")
# Tutorial: https://mlr-org.github.io/mlr-tutorial/release/html/
# We are on Github, feel free to contribute or star us: https://github.com/mlr-org/mlr
## Read Data
train = fread("data/train.csv", data.table = F)
test = fread("data/test.csv", data.table = F)
test$Response = 0
## store Id column and remove it from the train and test data
testId = test$Id
train$Id = test$Id = NULL
train$Product_Info_2 <- as.factor(train$Product_Info_2)
train$Product_Info_2_char = as.factor(substr(train$Product_Info_2, 1,1))
train$Product_Info_2_num = as.factor(substr(train$Product_Info_2, 2,2))
test$Product_Info_2_char = as.factor(substr(test$Product_Info_2, 1,1))
test$Product_Info_2_num = as.factor(substr(test$Product_Info_2, 2,2))
test$Product_Info_2 <- as.factor(test$Product_Info_2)
## create mlr task and convert factors to dummy features
trainTask = makeRegrTask(data = train, target = "Response")
trainTask = createDummyFeatures(trainTask)
testTask = makeRegrTask(data = test, target = "Response")
testTask = createDummyFeatures(testTask)
## create mlr learner
set.seed(123)
lrn = makeLearner("regr.xgboost")
setwd('/Users/ivanliu/Downloads/Prudential-Life-Insurance-Assessment')
rm(list = ls()); gc()
# Created by Giuseppe Casalicchio
library(Metrics)
library(Hmisc)
library(xgboost)
library(checkmate)
library(mlr)
library(data.table)
# Tutorial: https://mlr-org.github.io/mlr-tutorial/release/html/
# We are on Github, feel free to contribute or star us: https://github.com/mlr-org/mlr
## Read Data
train = fread("data/train.csv", data.table = F)
test = fread("data/test.csv", data.table = F)
test$Response = 0
## store Id column and remove it from the train and test data
testId = test$Id
train$Id = test$Id = NULL
train$Product_Info_2 <- as.factor(train$Product_Info_2)
test$Product_Info_2 <- as.factor(test$Product_Info_2)
## create mlr task and convert factors to dummy features
trainTask = makeRegrTask(data = train, target = "Response")
trainTask = createDummyFeatures(trainTask)
testTask = makeRegrTask(data = test, target = "Response")
testTask = createDummyFeatures(testTask)
## create mlr learner
set.seed(1)
lrn = makeLearner("regr.xgboost")
install.packages('mlr')
install.packages("mlr")
setwd('/Users/ivanliu/Downloads/Prudential-Life-Insurance-Assessment')
library(Metrics)
library(Hmisc)
library(xgboost)
library(checkmate)
library(mlr)
library(data.table)
load("data/cleaned_datasets.RData")
# load("data/cleaned_datasets_imputed.RData")
# load("data/cleaned_datasets_no_encoded.RData")
##################################################################
# 1.create mlr task and convert factors to dummy features ########
##################################################################
allTask = makeRegrTask(data = train[,-1], target = "Response")
trainTask = makeRegrTask(data = train_20[,-1], target = "Response")
validTask = makeRegrTask(data = validation_20[,-1], target = "Response")
testTask = makeRegrTask(data = test[,-1], target = "Response")
##############################
# 2.create mlr learner #######
##############################
set.seed(1989)
lrn = makeLearner("regr.xgboost")
